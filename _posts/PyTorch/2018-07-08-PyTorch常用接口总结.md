---
layout: post
title: PyTorch常用接口总结
category: 框架
tag: Deep-Learning
description: 总结了PyTorch中常用的Python接口
---

## 概述



### 1. 构造nn.Sequential

```python
# 以dict的方式构造nn.Sequential，指定每一层的name
classifier = nn.Sequential(OrderedDict([
  ('0', nn.Linear(640 * 7 * 7, 4200)),
  ('1', nn.ReLU(True)),
  ]))

# 以list的方式构造nn.Sequential，自动生成每一层的name
layer_list = [nn.Linear(640 * 7 * 7, 4200), nn.ReLU(True)]
classifier = nn.Sequential(*layer_list)# layer的命名规则为'0'->Linear, '1'->ReLU

# 如果classifier是Module的成员变量，那么layer的命名规则为'classifier.0'->Linear, 'classifier.1'->ReLU
self.classifier = nn.Sequential(*layer_list)
```



### 以dict的方式访问nn.Sequential中的Module

```python
# The modules are stored as an ordered dictionary in nn.Sequential. You can directly access them with ._modules.

model = nn.Sequential(OrderedDict([
            ("layer_one", nn.Linear(1, 10)), ("layer_two", nn.Linear(10, 3))
        ]))
# 以dict的方式访问nn.Sequential中的Module
model._modules["layer_one"]
model._modules["layer_two"]

# 以list的方式访问nn.Sequential中的Module
modules_list = list(model._modules.values())

# 用index访问
fc = model[0]
```



### 访问Module或者Sequential中的可学习参数

```python
module = nn.Linear(1, 10)
module.state_dict()# dict类型，key的形式如: '0.weight', '0.bias'
# 或者
params = module.parameters()

# key的命名形式有如下特点，如果是成员变量，则会带有成员变量名，具体如下
self.classifier = nn.Sequential(*layer_list) # key: 'classifier.0.weight', 'classifier.0.bias'
```



### 访问Model (Module类型) 中的成员Module

```python
# 包括model中的所有Module对象，例如Module对象中的成员Module对象，迭代类型<class 'generator'>
modules = model.modules() 

# 输出model中所有Module对象的name及module
for name, module in model.named_modules():
  print(name, module)
```



### 常用网络层容器

```python
nn.Sequential # 按顺序调用包含在其中的Module对象
nn.ModuleList # 访问方式接近python中的list
nn.ModuleDict
nn.ParameterList
nn.ParameterDict
```






















